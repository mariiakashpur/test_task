{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from keras import metrics\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to retrieve files that will be used for training. Let's store file contents and file names in two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33716 33716\n"
     ]
    }
   ],
   "source": [
    "def get_training_files(dir):\n",
    "    'Get relevant training data files from the folder.'\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if 'spam' in name or 'ham' in name:\n",
    "                r.append(os.path.join(root, name))\n",
    "    return r\n",
    "\n",
    "all_files = 'data'\n",
    "train_data = []\n",
    "labels = []\n",
    "for f in get_training_files(all_files):\n",
    "    with codecs.open(f, 'r', encoding='utf-8', errors='ignore') as fdata:\n",
    "        train_data.append(fdata.read().replace('\\n', ' ').replace('\\r', ''))\n",
    "        labels.append('spam') if 'spam' in f else labels.append('ham')\n",
    "\n",
    "print(len(train_data), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert the two lists to a Pandas dataframe which makes it easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(train_data, labels)), columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 33716\n",
      "Number of ham data points: 16545\n",
      "Number of spam data points: 17171\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points:\", df.shape[0])\n",
    "print(\"Number of ham data points:\", df[(df['label'] == 'ham')].shape[0])\n",
    "print(\"Number of spam data points:\", df[(df['label'] == 'spam')].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty balanced regarding labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to vectorize the text, i.e. convert it to a representation suitable for training a classifier. For this, we will use the TF-IDF vectorizer (Term Frequency — Inverse Document Frequency), an embedding technique which takes into account the importance of each term to a text. We will also consider bigrams (sequences of two words), lowercase the texts and eliminate stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1444138)\t0.012421620199937693\n",
      "  (0, 1570112)\t0.48445104864588057\n",
      "  (0, 374150)\t0.03888352178585067\n",
      "  (0, 487665)\t0.12697100370886735\n",
      "  (0, 677916)\t0.08458536474045623\n",
      "  (0, 1569849)\t0.0742571012466618\n",
      "  (0, 1488339)\t0.13200753728611364\n",
      "  (0, 670361)\t0.0681761191221993\n",
      "  (0, 999921)\t0.05835243300525048\n",
      "  (0, 1385691)\t0.1054244482004252\n",
      "  (0, 1638358)\t0.09543283458501586\n",
      "  (0, 1604651)\t0.13200753728611364\n",
      "  (0, 625015)\t0.12062572182204993\n",
      "  (0, 1182916)\t0.11644619149091767\n",
      "  (0, 347325)\t0.06549938788990445\n",
      "  (0, 1533233)\t0.06686372864277365\n",
      "  (0, 1386712)\t0.04391144418757016\n",
      "  (0, 1066186)\t0.042866829031315064\n",
      "  (0, 602080)\t0.06916127581364155\n",
      "  (0, 504888)\t0.07301390253617075\n",
      "  (0, 713018)\t0.04502376539637998\n",
      "  (0, 1056263)\t0.0412652637854407\n",
      "  (0, 957270)\t0.1047144480924745\n",
      "  (0, 1380031)\t0.09253095963057424\n",
      "  (0, 601061)\t0.06253526739816576\n",
      "  :\t:\n",
      "  (30343, 1352948)\t0.23983095625783568\n",
      "  (30343, 1074571)\t0.07994365208594521\n",
      "  (30343, 1270615)\t0.07994365208594521\n",
      "  (30343, 1494049)\t0.07994365208594521\n",
      "  (30343, 1283163)\t0.07994365208594521\n",
      "  (30343, 336001)\t0.07994365208594521\n",
      "  (30343, 1112066)\t0.07994365208594521\n",
      "  (30343, 447830)\t0.07994365208594521\n",
      "  (30343, 55656)\t0.15988730417189043\n",
      "  (30343, 1074572)\t0.07994365208594521\n",
      "  (30343, 1374189)\t0.15988730417189043\n",
      "  (30343, 1312403)\t0.07994365208594521\n",
      "  (30343, 1074573)\t0.07994365208594521\n",
      "  (30343, 853732)\t0.07994365208594521\n",
      "  (30343, 1270471)\t0.07994365208594521\n",
      "  (30343, 1525252)\t0.07994365208594521\n",
      "  (30343, 1610502)\t0.07994365208594521\n",
      "  (30343, 137087)\t0.07994365208594521\n",
      "  (30343, 1500415)\t0.07994365208594521\n",
      "  (30343, 1541403)\t0.07994365208594521\n",
      "  (30343, 580231)\t0.07994365208594521\n",
      "  (30343, 1543547)\t0.07994365208594521\n",
      "  (30343, 540153)\t0.07994365208594521\n",
      "  (30343, 466438)\t0.07994365208594521\n",
      "  (30343, 341189)\t0.07994365208594521\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=1) # shuffle rows in dataframe\n",
    "df.head(10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size = 0.1, random_state = 1)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function for printing out evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y, pred):\n",
    "        \"\"\"\n",
    "        Use sklearn for model evaluation\n",
    "        :param y: gold labels\n",
    "        :param pred: predicted labels\n",
    "        \"\"\"\n",
    "        print(\"accuracy: \", accuracy_score(y, pred))\n",
    "        print(\"recall: \", recall_score(y, pred, average='weighted'))\n",
    "        print(\"precision: \", precision_score(y, pred, average='weighted'))\n",
    "        print(\"f1_score macro: \", f1_score(y, pred, average='macro'))\n",
    "        print(\"f1_score micro: \", f1_score(y, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try several different classification algorithms. \n",
    "\n",
    "We can start with **Naive Bayes**, which is often used as a baseline for spam filtering. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.384 s\n",
      "Prediction time: 0.036 s\n",
      "Results for Naive Bayes: \n",
      "accuracy:  0.9911032028469751\n",
      "recall:  0.9911032028469751\n",
      "precision:  0.9911200136450838\n",
      "f1_score macro:  0.991101950745128\n",
      "f1_score micro:  0.9911032028469751\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "t0=time()\n",
    "clf.fit(X_train, y_train)  \n",
    "print(\"Training time:\", round(time() - t0, 3), \"s\")\n",
    "t1 = time()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction time:\", round(time() - t1, 3), \"s\")\n",
    "print(\"Results for Naive Bayes: \")\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can try **Logistic Regression**, a binary classifier that takes a linear combination of features and applies non-linear function (sigmoid) to it. Logistic regression provides lots of ways to regularize the model, and you don’t have to worry as much about your features being correlated like in Naive Bayes, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bobrusha/env3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.616 s\n",
      "Prediction time: 0.006 s\n",
      "Results for Logistic Regression: \n",
      "accuracy:  0.9845788849347569\n",
      "recall:  0.9845788849347569\n",
      "precision:  0.9848221925751746\n",
      "f1_score macro:  0.9845726110704431\n",
      "f1_score micro:  0.9845788849347569\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression()\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training time:\", round(time() - t0, 3), \"s\") # the time would be round to 3 decimal in seconds\n",
    "t1 = time()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction time:\", round(time() - t1, 3), \"s\")\n",
    "print(\"Results for Logistic Regression: \")\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-nearest neighbors** can be useful in case of nonlinear data,  training phase is fast, but testing can be slow as it requires large memory for storing the entire training dataset for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.086 s\n",
      "Prediction time: 8.506 s\n",
      "Results for K-nearest neighbours: \n",
      "accuracy:  0.982502965599051\n",
      "recall:  0.982502965599051\n",
      "precision:  0.982524519915733\n",
      "f1_score macro:  0.9825024100654853\n",
      "f1_score micro:  0.982502965599051\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training time:\", round(time() - t0, 3), \"s\")\n",
    "t1 = time()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction time:\", round(time() - t1, 3), \"s\")\n",
    "print(\"Results for K-nearest neighbours: \")\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support vector machines** is another popular algorithm which chooses the decision boundary that maximizes the distance from the nearest data points of all the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 941.163 s\n",
      "Prediction time: 28.025 s\n",
      "Results for SVM: \n",
      "accuracy:  0.9916963226571768\n",
      "recall:  0.9916963226571768\n",
      "precision:  0.9917207114158839\n",
      "f1_score macro:  0.99169503422579\n",
      "f1_score micro:  0.9916963226571768\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear') \n",
    "t0=time()\n",
    "clf.fit(X_train, y_train)  \n",
    "print(\"Training time:\", round(time() - t0, 3), \"s\")\n",
    "t1 = time()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction time:\", round(time() - t1, 3), \"s\")\n",
    "print(\"Results for SVM: \")\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try training a **neural network**. Let's build a simple feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 2000\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(df['label'])\n",
    "tokenizer = Tokenizer(num_words=num_max)\n",
    "tokenizer.fit_on_texts(df['text']) # set up internal vocab using all words from training data and attach indices to them\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], tags, test_size = 0.1, random_state = 1)\n",
    "\n",
    "\n",
    "mat_texts_tr = tokenizer.texts_to_matrix(X_train,mode='count')\n",
    "mat_texts_tst = tokenizer.texts_to_matrix(X_test,mode='count')\n",
    "\n",
    "train_data_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_data_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify the architecture of the model. It will be a simple sequential model that will use an input layer with 2000 input neurons (this number was chosen experimentally), two hidden layers for internal transformation and one output layer that gives us a scalar prediction value indicating if we have spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(num_max,)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc',metrics.binary_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 512)               1024512   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,156,097\n",
      "Trainable params: 1,156,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21240 samples, validate on 9104 samples\n",
      "Epoch 1/10\n",
      "21240/21240 [==============================] - 21s 966us/step - loss: 0.0940 - acc: 0.9715 - binary_accuracy: 0.9715 - val_loss: 0.0612 - val_acc: 0.9820 - val_binary_accuracy: 0.9820\n",
      "Epoch 2/10\n",
      "21240/21240 [==============================] - 20s 942us/step - loss: 0.0291 - acc: 0.9926 - binary_accuracy: 0.9926 - val_loss: 0.0423 - val_acc: 0.9881 - val_binary_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "21240/21240 [==============================] - 15s 698us/step - loss: 0.0270 - acc: 0.9943 - binary_accuracy: 0.9943 - val_loss: 0.0450 - val_acc: 0.9877 - val_binary_accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "21240/21240 [==============================] - 16762s 789ms/step - loss: 0.0102 - acc: 0.9978 - binary_accuracy: 0.9978 - val_loss: 0.0502 - val_acc: 0.9878 - val_binary_accuracy: 0.9878\n",
      "Epoch 5/10\n",
      "21240/21240 [==============================] - 23s 1ms/step - loss: 0.0098 - acc: 0.9980 - binary_accuracy: 0.9980 - val_loss: 0.1601 - val_acc: 0.9781 - val_binary_accuracy: 0.9781\n",
      "Epoch 6/10\n",
      "21240/21240 [==============================] - 28s 1ms/step - loss: 0.0141 - acc: 0.9979 - binary_accuracy: 0.9979 - val_loss: 0.0551 - val_acc: 0.9892 - val_binary_accuracy: 0.9892\n",
      "Epoch 7/10\n",
      "21240/21240 [==============================] - 22s 1ms/step - loss: 0.0085 - acc: 0.9990 - binary_accuracy: 0.9990 - val_loss: 0.0709 - val_acc: 0.9888 - val_binary_accuracy: 0.9888\n",
      "Epoch 8/10\n",
      "21240/21240 [==============================] - 22s 1ms/step - loss: 0.0082 - acc: 0.9986 - binary_accuracy: 0.9986 - val_loss: 0.0746 - val_acc: 0.9876 - val_binary_accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "21240/21240 [==============================] - 24s 1ms/step - loss: 0.0192 - acc: 0.9966 - binary_accuracy: 0.9966 - val_loss: 0.0634 - val_acc: 0.9886 - val_binary_accuracy: 0.9886\n",
      "Epoch 10/10\n",
      "21240/21240 [==============================] - 22s 1ms/step - loss: 0.0101 - acc: 0.9987 - binary_accuracy: 0.9987 - val_loss: 0.0706 - val_acc: 0.9879 - val_binary_accuracy: 0.9879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1329d81d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_simple_model()\n",
    "m.fit(mat_texts_tr,y_train,batch_size=32,epochs=10,verbose=1,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the performance of the network on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 1s 180us/step\n",
      "['loss', 'acc', 'binary_accuracy']\n",
      "Test result:  [0.08104704655032939, 0.9899169632265717, 0.9899169632265717]\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(mat_texts_tst,y_test)\n",
    "print(m.metrics_names)\n",
    "print('Test result: ', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I think I will choose the Naive Bayes algorithm for this task. The training and prediction time is quite fast, and the accuracy, precision and recall are very good on the test data (over 99%). SVM and neural network are also not bad, but the training time is just too long compared to Naive Bayes. Also, Naive Bayes is quite intuitive to understand and so it's easier to interpret results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
